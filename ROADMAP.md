# üìà Roadmap - Projeto de Pipeline de Dados de Not√≠cias

---

## ‚úÖ Etapa 1: ETL Manual e Simples
- [x] Coletar dados de uma API de not√≠cias (NewsAPI).
- [x] Filtrar not√≠cias relacionadas a acidentes de carro com √°lcool.
- [x] Salvar os dados localmente (`.json` ou `.csv`).
- [x] Fazer upload dos arquivos para o Amazon S3 usando `boto3`.

---

## ‚úÖ Etapa 2: Automa√ß√£o com Python Puro
- [x] Criar `main_etl.py` para orquestrar o processo de ETL completo.
- [x] Automatizar a execu√ß√£o manual do ETL.
- [x] **(AWS Lambda foi deixado de lado para focar no ambiente local e facilitar o uso do projeto via GitHub).**

---

## ‚úÖ Etapa 3: Orquestra√ß√£o com Apache Airflow
- [x] Subir PostgreSQL e Apache Airflow localmente com Docker Compose.
- [x] Configurar Airflow com conex√£o no PostgreSQL.
- [x] Criar DAG no Airflow para orquestrar o ETL.
- [x] Rodar e monitorar o pipeline via interface web do Airflow.

---

## ‚úÖ Etapa 4: Valida√ß√£o e Qualidade dos Dados
- [x] Adicionar valida√ß√µes nos dados:
  - Conferir se as colunas esperadas est√£o presentes.
  - Tratar valores nulos ou inconsist√™ncias.
- [x] Criar logs ou relat√≥rios simples de erros detectados.
- [x] Adicionar novas API's de not√≠cias (GNews, NewsData).

---

## ‚úÖ Etapa 5: Refatora√ß√£o para Arquitetura SOLID
- [x] Implementar interfaces base (BaseExtractor, BaseTransformer, BaseLoader).
- [x] Aplicar princ√≠pio da Responsabilidade √önica separando extratores, transformadores e carregadores.
- [x] Melhorar gerenciamento de configura√ß√£o com padr√£o Singleton.
- [x] Padronizar o sistema de logging em toda a aplica√ß√£o.
- [x] Criar estrutura modular para facilitar extens√µes futuras.
- [x] Refatorar Docker e Airflow para trabalhar com a nova arquitetura.

---

## üßä Etapa 6: Armazenamento Estruturado
- [x] Criar tabelas e schemas apropriados para consultas SQL no PostgreSQL.
- [ ] Integrar o pipeline com Amazon Redshift, Athena ou Snowflake.
- [ ] Implementar sistema de migra√ß√£o de esquema (usando Alembic ou similar).

---

## üß™ Etapa 7: Testes Automatizados
- [ ] Implementar testes unit√°rios para todos os componentes.
- [ ] Configurar testes de integra√ß√£o para o pipeline completo.
- [ ] Adicionar verifica√ß√µes de cobertura de c√≥digo.
- [ ] Configurar CI/CD para execu√ß√£o autom√°tica de testes.

---

## üìä Etapa 8: Visualiza√ß√£o
- [x] Conectar os dados no S3 ou banco de dados no Metabase, Superset ou Power BI.
- [ ] Criar dashboards de an√°lise:
  - N√∫mero de acidentes por estado.
  - N√∫mero de acidentes por data.
  - Outros insights relevantes.
- [ ] Desenvolver API REST para acesso aos dados processados.

---

## üì¶ Etapa 9: Empacotamento e Distribui√ß√£o
- [ ] Configurar setup.py para transformar o projeto em pacote Python.
- [ ] Publicar pacote no PyPI para instala√ß√£o via pip.
- [ ] Criar imagens Docker pr√©-configuradas para facilitar implanta√ß√£o.
- [x] Adicionar documenta√ß√£o autom√°tica com Sphinx ou MkDocs.

---

## üßµ Etapa 10: Streaming de Dados (Extra/Opcional)
- [ ] Integrar Kafka ou Kinesis para ingest√£o cont√≠nua de not√≠cias em tempo real.
- [ ] Adaptar o ETL para consumir e processar dados em streaming.
- [ ] Implementar an√°lise em tempo real com Spark Streaming ou Flink.

---

# üéØ Observa√ß√£o
O projeto foi desenhado para ser totalmente funcional com simples `git clone`, `docker compose up`, e execu√ß√£o do Airflow localmente, sem necessidade de configura√ß√£o externa (ex: AWS Lambda, IAM roles).

A arquitetura SOLID implementada facilita a manuten√ß√£o, testabilidade e extensibilidade do c√≥digo, permitindo adicionar novas fontes de dados ou destinos com o m√≠nimo de altera√ß√µes no c√≥digo existente.

