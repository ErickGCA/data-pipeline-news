Metadata-Version: 2.4
Name: news_etl_pipeline
Version: 0.1.0
Summary: Pipeline ETL para extra√ß√£o e an√°lise de not√≠cias
Author-email: √ârick GCA <example@example.com>
Description-Content-Type: text/markdown

# News ETL Pipeline with SOLID Architecture

This project implements an ETL (Extract, Transform, Load) pipeline to process news related to car accidents involving alcohol consumption, following SOLID principles for improved maintainability and extensibility.

---

## Project Structure

The project follows a SOLID architecture with clear separation of responsibilities:

```
üìÅ pipelines-news/
‚îú‚îÄ‚îÄ üìÅ src/                           # Source code with SOLID architecture
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ etl/                       # Core ETL components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ extractors/            # Data extraction components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base_extractor.py  # Interface for all extractors
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ news_api_extractor.py # NewsAPI implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ gnews_extractor.py # GNews API implementation
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ transformers/          # Data transformation components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base_transformer.py # Interface for all transformers
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ news_transformer.py # Filtering and relevance scoring
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ loaders/               # Data loading components
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ base_loader.py     # Interface for all loaders
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ postgres_loader.py # Loads data to PostgreSQL
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ s3_uploader.py     # Uploads data to Amazon S3
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                     # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ config.py              # Centralized configuration management
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ logger.py              # Standardized logging system
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ database.py            # Database connection handling
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docker/                        # Infrastructure configuration
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ services/                  # Service-specific configurations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ airflow/               # Airflow configuration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile         # Custom Airflow image
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ requirements.txt   # Python dependencies
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ postgres/              # PostgreSQL configuration
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÅ init-scripts/      # Database initialization scripts
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ üìÑ init-db.sql    # SQL setup script
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ dags/                      # Airflow DAGs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ pipelines/             # Pipeline definitions
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ news_etl_dag.py    # New SOLID-based DAG implementation
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ etl_pipeline_dag.py # Legacy DAG (preserved for reference)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/                      # Data storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                   # Raw extracted data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ processed/             # Transformed data
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ logs/                      # Airflow logs
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ plugins/                   # Airflow plugins
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ docker-compose.yml         # Docker services configuration
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ README.md                  # Docker setup documentation
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md                      # Main project documentation
‚îî‚îÄ‚îÄ üìÑ requirements.txt               # Python dependencies for local development
```

> **Note**: The legacy structure has been preserved for backward compatibility while transitioning to the new architecture.

---

## How to Run

### Local Environment

1. Clone the repository:
   ```bash
   git clone https://github.com/ErickGCA/data-pipeline-news.git
   cd pipelines-news
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure your `.env` file:
   ```ini
   AWS_ACCESS_KEY=...
   AWS_SECRET_KEY=...
   AWS_REGION=...
   S3_BUCKET=...
   NEWS_API_KEY=...
   GNEWS_API_KEY=...
   NEWSDATA_API_KEY=...
   POSTGRES_USER=...
   POSTGRES_PASSWORD=...
   POSTGRES_DB=...
   POSTGRES_PORT=...
   POSTGRES_HOST=...
   LOG_TO_FILE=True
   ```

---

### Environment with Docker and Airflow

1. Start Docker, then launch the containers:
   ```bash
   cd docker
   docker-compose up -d
   ```

2. Access Airflow via browser:
   [http://localhost:8080](http://localhost:8080)

3. In Airflow:
   - Find the `news_etl_pipeline` DAG (new SOLID architecture)
   - Enable and run the pipeline through the interface

---

## Architecture Benefits

The new SOLID architecture provides several advantages:

- **Single Responsibility**: Each component has one focused responsibility
- **Open/Closed**: New extractors, transformers, or loaders can be added without modifying existing code
- **Liskov Substitution**: Components are interchangeable as long as they follow the interfaces
- **Interface Segregation**: Clean interfaces for each component type
- **Dependency Inversion**: High-level modules depend on abstractions, not implementations

This makes the codebase more maintainable, testable, and extensible.

---

## Notes

- Ensure that the AWS credentials are correct to avoid errors when uploading to S3
- Make sure all API keys are correct
- Ensure PostgreSQL credentials are correct
- The project requires Docker with docker-compose for the full infrastructure
